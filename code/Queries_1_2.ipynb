{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dff3f87-0fa6-4e54-b429-68d1e69984fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|   Age Group|Incident Count|\n",
      "+------------+--------------+\n",
      "|      Adults|        121052|\n",
      "|Young Adults|         33588|\n",
      "|    Children|         10825|\n",
      "|     Elderly|          5985|\n",
      "+------------+--------------+\n",
      "\n",
      "DataFrame API execution time: 18.385648727416992 seconds\n",
      "Adults: 121093 incidents\n",
      "Young Adults: 33605 incidents\n",
      "Children: 10830 incidents\n",
      "Elderly: 5985 incidents\n",
      "RDD API execution time: 7.3916239738464355 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Paths for the two CSV files\n",
    "csv_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "csv_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 1 - DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", 4) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the first CSV file\n",
    "csv_df_1 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csv_path_1)\n",
    "\n",
    "# Load the second CSV file\n",
    "csv_df_2 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csv_path_2)\n",
    "\n",
    "# csv_df_1 = csv_df_1.withColumnRenamed(\"AREA \", \"AREA\")\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_df = csv_df_1.union(csv_df_2)\n",
    "\n",
    "combined_df_1 = combined_df.filter((col('LAT')!= 0) & (col('LON')!= 0))\n",
    "\n",
    "# Filter for aggravated assault crimes\n",
    "aggravated_assault_df = combined_df_1.filter(combined_df[\"Crm Cd Desc\"].rlike(\"(?i).*aggravated assault*\"))\n",
    "\n",
    "# Add age group column\n",
    "age_grouped_df = aggravated_assault_df.withColumn(\n",
    "    \"Age Group\",\n",
    "    when((col(\"Vict Age\") > 0) & (col(\"Vict Age\") < 18), \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\") > 64, \"Elderly\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "age_group_counts_df_1 = age_grouped_df.filter(col(\"Age Group\") != \"Unknown\")\n",
    "\n",
    "# Group by age group and count incidents\n",
    "age_group_counts_df_2 = age_group_counts_df_1.groupBy(\"Age Group\").agg(count(\"*\").alias(\"Incident Count\"))\n",
    "\n",
    "# Sort by incident count\n",
    "sorted_age_groups_df = age_group_counts_df_2.orderBy(col(\"Incident Count\").desc())\n",
    "\n",
    "# Show results\n",
    "sorted_age_groups_df.show()\n",
    "\n",
    "# Measure execution time\n",
    "end_time = time.time()\n",
    "df_execution_time = end_time - start_time\n",
    "print(f\"DataFrame API execution time: {df_execution_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 1 - RDD API\") \\\n",
    "    .config(\"spark.executor.instances\", 4) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load both CSV files into RDDs\n",
    "csv_rdd_1 = spark.read.csv(csv_path_1, header=True, inferSchema=True).rdd\n",
    "csv_rdd_2 = spark.read.csv(csv_path_2, header=True, inferSchema=True).rdd\n",
    "\n",
    "# Combine the RDDs\n",
    "combined_rdd = csv_rdd_1.union(csv_rdd_2)\n",
    "\n",
    "# Filter for aggravated assault crimes\n",
    "filtered_rdd = combined_rdd.filter(lambda row: row['LAT'] != 0 and row['LON'] != 0)\n",
    "\n",
    "filtered_rdd_1 = combined_rdd.filter(lambda row: 'aggravated assault' in (row['Crm Cd Desc'] or \"\").lower())\n",
    "\n",
    "# Map age groups\n",
    "age_grouped_rdd = filtered_rdd_1.map(lambda row: (\n",
    "    \"Children\" if 1 <= row['Vict Age'] < 18 else\n",
    "    \"Young Adults\" if 18 <= row['Vict Age'] <= 24 else\n",
    "    \"Adults\" if 25 <= row['Vict Age'] <= 64 else\n",
    "    \"Elderly\" if row['Vict Age'] >= 65 else\n",
    "    \"Unknown\", 1  # Handle any invalid or missing ages as \"Unknown\"\n",
    "))\n",
    "\n",
    "age_grouped_rdd_1 = age_grouped_rdd.filter(lambda row: row[0] != \"Unknown\")\n",
    "\n",
    "\n",
    "# Reduce by key to get counts\n",
    "age_group_counts_rdd = age_grouped_rdd_1.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Sort by incident count\n",
    "sorted_age_groups_rdd = age_group_counts_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Collect results\n",
    "results = sorted_age_groups_rdd.collect()\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "rdd_execution_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "for age_group, count in results:\n",
    "    print(f\"{age_group}: {count} incidents\")\n",
    "\n",
    "print(f\"RDD API execution time: {rdd_execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b4b186c-bdab-49a1-a1ec-40cde92a6598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 19.646901607513428 seconds\n",
      "+----+-----------+--------------------+\n",
      "|Year|AREA NAME  |ClosedCasePercentage|\n",
      "+----+-----------+--------------------+\n",
      "|2010|Rampart    |32.85090742017      |\n",
      "|2010|Olympic    |31.515289821999087  |\n",
      "|2010|Harbor     |29.36028339237341   |\n",
      "|2011|Olympic    |35.03192688118192   |\n",
      "|2011|Rampart    |32.500296103280824  |\n",
      "|2011|Harbor     |28.516260162601625  |\n",
      "|2012|Olympic    |34.295435879385195  |\n",
      "|2012|Rampart    |32.461037450569904  |\n",
      "|2012|Harbor     |29.534834324553948  |\n",
      "|2013|Olympic    |33.58217940999398   |\n",
      "|2013|Rampart    |32.1060382916053    |\n",
      "|2013|Harbor     |29.70696405267529   |\n",
      "|2014|Van Nuys   |32.00295639320029   |\n",
      "|2014|West Valley|31.512710797885727  |\n",
      "|2014|Mission    |31.21740874448456   |\n",
      "|2015|Van Nuys   |32.265140677157845  |\n",
      "|2015|Mission    |30.466622852314334  |\n",
      "|2015|Foothill   |30.353001803658852  |\n",
      "|2016|Van Nuys   |32.1880650994575    |\n",
      "|2016|West Valley|31.404702970297027  |\n",
      "|2016|Foothill   |29.908647228131645  |\n",
      "|2017|Van Nuys   |32.04915003695491   |\n",
      "|2017|Mission    |31.055387158996968  |\n",
      "|2017|Foothill   |30.469700657094183  |\n",
      "|2018|Foothill   |30.731346958877126  |\n",
      "|2018|Mission    |30.730034296913278  |\n",
      "|2018|Van Nuys   |28.905206942590123  |\n",
      "|2019|Mission    |30.719878207916484  |\n",
      "|2019|West Valley|30.57529223011689   |\n",
      "|2019|Foothill   |29.208050182958704  |\n",
      "|2020|West Valley|30.804455445544555  |\n",
      "|2020|Mission    |30.328549401020044  |\n",
      "|2020|Harbor     |29.780741410488247  |\n",
      "|2021|Mission    |30.555555555555557  |\n",
      "|2021|West Valley|29.05611645926274   |\n",
      "|2021|Foothill   |28.19376964847099   |\n",
      "|2022|West Valley|26.536367172306498  |\n",
      "|2022|Harbor     |26.337538060026098  |\n",
      "|2022|Topanga    |26.236786469344608  |\n",
      "|2023|Foothill   |26.750524109014673  |\n",
      "|2023|Topanga    |26.538022616453986  |\n",
      "|2023|Mission    |25.662731120516817  |\n",
      "|2024|Foothill   |18.667786174083403  |\n",
      "|2024|77th Street|17.6147382029735    |\n",
      "|2024|Mission    |17.187827911857294  |\n",
      "+----+-----------+--------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, row_number, desc,  year, lower, to_date \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 2 - Top Police Divisions by Closed Cases\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load Datasets\n",
    "stations_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "crime_2010_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_2020_present_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load Police Stations Data\n",
    "stations_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(stations_path)\n",
    "\n",
    "# Load Crime Data (2010-2019)\n",
    "crime_2010_2019_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(crime_2010_2019_path)\n",
    "\n",
    "# Load Crime Data (2020-Present)\n",
    "crime_2020_present_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(crime_2020_present_path)\n",
    "\n",
    "# Step 3: Combine Crime Datasets\n",
    "combined_crime_df_1 = crime_2010_2019_df.union(crime_2020_present_df)\n",
    "\n",
    "combined_crime_df = combined_crime_df_1 .filter(col(\"Status\") == \"CC\")\n",
    "\n",
    "combined_crime_df = combined_crime_df_1.filter((col('LAT')!= 0) & (col('LON')!= 0))\n",
    "\n",
    "result_df = stations_df.join(\n",
    "    combined_crime_df,\n",
    "    lower(stations_df['DIVISION']) == lower(combined_crime_df['AREA NAME']),\n",
    "    'inner'\n",
    ")\n",
    "\n",
    "# Add a Year column and filter only resolved cases\n",
    "df_with_year = result_df.withColumn(\"Year\", year(to_date(col(\"DATE OCC\"), \"MM/dd/yyyy\")))\n",
    "\n",
    "df_with_year_resolved = df_with_year.filter((col(\"Status Desc\") != \"UNK\") & (col(\"Status Desc\") != \"Invest Cont\"))\n",
    "\n",
    "# Step 3: Calculate Total and Closed Cases per Year and Division\n",
    "total_cases_df = df_with_year.groupBy(\"Year\", \"AREA NAME\").agg(\n",
    "    F.count(\"*\").alias(\"TotalCases\")\n",
    ")\n",
    "\n",
    "closed_cases_df = df_with_year_resolved.groupBy(\"Year\", \"AREA NAME\").agg(\n",
    "    F.count(\"*\").alias(\"ClosedCases\")\n",
    ")\n",
    "\n",
    "# Step 4: Join the Total Cases and Closed Cases DataFrames\n",
    "cases_df = total_cases_df.join(closed_cases_df, [\"Year\", \"AREA NAME\"], \"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Calculate the Closed Case Percentage\n",
    "cases_df = cases_df.withColumn(\n",
    "    \"ClosedCasePercentage\", \n",
    "    (F.col(\"ClosedCases\") / F.col(\"TotalCases\")) * 100\n",
    ")\n",
    "\n",
    "# Step 6: Rank divisions within each year based on Closed Case Percentage\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(F.col(\"ClosedCasePercentage\").desc())\n",
    "ranked_df = cases_df.withColumn(\"Rank\", F.row_number().over(window_spec))\n",
    "\n",
    "# Step 7: Filter top 3 divisions for each year\n",
    "top_3_df = ranked_df.filter(F.col(\"Rank\") <= 3)\n",
    "\n",
    "# Step 8: Order by year and rank\n",
    "result_df_1 = top_3_df.orderBy(\"Year\", \"Rank\")\n",
    "\n",
    "result_df = result_df_1.select(\"Year\", \"AREA NAME\", \"ClosedCasePercentage\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution Time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# Show the results\n",
    "result_df.show(70, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6245e00b-9d4f-48a8-804d-7dfd5a2dafae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------------------+\n",
      "|Year|Division   |ClosedCasePercentage|\n",
      "+----+-----------+--------------------+\n",
      "|2010|Rampart    |32.85090742017      |\n",
      "|2010|Olympic    |31.515289821999087  |\n",
      "|2010|Harbor     |29.36028339237341   |\n",
      "|2011|Olympic    |35.03192688118192   |\n",
      "|2011|Rampart    |32.500296103280824  |\n",
      "|2011|Harbor     |28.516260162601625  |\n",
      "|2012|Olympic    |34.295435879385195  |\n",
      "|2012|Rampart    |32.461037450569904  |\n",
      "|2012|Harbor     |29.534834324553948  |\n",
      "|2013|Olympic    |33.58217940999398   |\n",
      "|2013|Rampart    |32.1060382916053    |\n",
      "|2013|Harbor     |29.70696405267529   |\n",
      "|2014|Van Nuys   |32.00295639320029   |\n",
      "|2014|West Valley|31.512710797885727  |\n",
      "|2014|Mission    |31.21740874448456   |\n",
      "|2015|Van Nuys   |32.265140677157845  |\n",
      "|2015|Mission    |30.466622852314334  |\n",
      "|2015|Foothill   |30.353001803658852  |\n",
      "|2016|Van Nuys   |32.1880650994575    |\n",
      "|2016|West Valley|31.404702970297027  |\n",
      "|2016|Foothill   |29.908647228131645  |\n",
      "|2017|Van Nuys   |32.04915003695491   |\n",
      "|2017|Mission    |31.055387158996968  |\n",
      "|2017|Foothill   |30.469700657094183  |\n",
      "|2018|Foothill   |30.731346958877126  |\n",
      "|2018|Mission    |30.730034296913278  |\n",
      "|2018|Van Nuys   |28.905206942590123  |\n",
      "|2019|Mission    |30.719878207916484  |\n",
      "|2019|West Valley|30.57529223011689   |\n",
      "|2019|Foothill   |29.208050182958704  |\n",
      "|2020|West Valley|30.804455445544555  |\n",
      "|2020|Mission    |30.328549401020044  |\n",
      "|2020|Harbor     |29.780741410488247  |\n",
      "|2021|Mission    |30.555555555555557  |\n",
      "|2021|West Valley|29.05611645926274   |\n",
      "|2021|Foothill   |28.19376964847099   |\n",
      "|2022|West Valley|26.536367172306498  |\n",
      "|2022|Harbor     |26.337538060026098  |\n",
      "|2022|Topanga    |26.236786469344608  |\n",
      "|2023|Foothill   |26.750524109014673  |\n",
      "|2023|Topanga    |26.538022616453986  |\n",
      "|2023|Mission    |25.662731120516817  |\n",
      "|2024|Foothill   |18.667786174083403  |\n",
      "|2024|77th Street|17.6147382029735    |\n",
      "|2024|Mission    |17.187827911857294  |\n",
      "+----+-----------+--------------------+\n",
      "\n",
      "Query execution time: 0.05 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Register DataFrames as temporary views\n",
    "stations_df.createOrReplaceTempView(\"stations\")\n",
    "combined_crime_df.createOrReplaceTempView(\"combined_crime\")\n",
    "\n",
    "# Step 2: Run the SQL query\n",
    "query = \"\"\"\n",
    "WITH ResolvedCases AS (\n",
    "    SELECT \n",
    "        s.DIVISION, \n",
    "        c.*, \n",
    "        CASE \n",
    "            WHEN c.`Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 \n",
    "            ELSE 0 \n",
    "        END AS is_resolved,\n",
    "        YEAR(TO_DATE(c.`DATE OCC`, 'MM/dd/yyyy')) AS Year\n",
    "    FROM \n",
    "        stations s\n",
    "    INNER JOIN \n",
    "        combined_crime c\n",
    "    ON \n",
    "        LOWER(s.DIVISION) = LOWER(c.`AREA NAME`)\n",
    "),\n",
    "TotalCases AS (\n",
    "    SELECT \n",
    "        Year, \n",
    "        `AREA NAME` AS Division, \n",
    "        COUNT(*) AS TotalCases\n",
    "    FROM \n",
    "        ResolvedCases\n",
    "    GROUP BY \n",
    "        Year, `AREA NAME`\n",
    "),\n",
    "ClosedCases AS (\n",
    "    SELECT \n",
    "        Year, \n",
    "        `AREA NAME` AS Division, \n",
    "        COUNT(*) AS ClosedCases\n",
    "    FROM \n",
    "        ResolvedCases\n",
    "    WHERE \n",
    "        is_resolved = 1\n",
    "    GROUP BY \n",
    "        Year, `AREA NAME`\n",
    "),\n",
    "CasesWithPercentage AS (\n",
    "    SELECT \n",
    "        t.Year, \n",
    "        t.Division, \n",
    "        t.TotalCases, \n",
    "        COALESCE(c.ClosedCases, 0) AS ClosedCases, \n",
    "        (COALESCE(c.ClosedCases, 0) / t.TotalCases) * 100 AS ClosedCasePercentage\n",
    "    FROM \n",
    "        TotalCases t\n",
    "    LEFT JOIN \n",
    "        ClosedCases c\n",
    "    ON \n",
    "        t.Year = c.Year AND t.Division = c.Division\n",
    "),\n",
    "RankedDivisions AS (\n",
    "    SELECT \n",
    "        Year, \n",
    "        Division, \n",
    "        TotalCases, \n",
    "        ClosedCases, \n",
    "        ClosedCasePercentage,\n",
    "        ROW_NUMBER() OVER (PARTITION BY Year ORDER BY ClosedCasePercentage DESC) AS Rank\n",
    "    FROM \n",
    "        CasesWithPercentage\n",
    ")\n",
    "SELECT \n",
    "    Year, \n",
    "    Division, \n",
    "    ClosedCasePercentage \n",
    "FROM \n",
    "    RankedDivisions\n",
    "WHERE \n",
    "    Rank <= 3\n",
    "ORDER BY \n",
    "    Year, Rank\n",
    "\"\"\"\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Show the results\n",
    "result_df.show(70, truncate=False)\n",
    "\n",
    "# Print the execution time\n",
    "print(f\"Query execution time: {execution_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1ee7746-618a-44fd-a9fe-b3274a3089d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time (Parquet): 0.57 seconds\n",
      "+----+-----------+--------------------+\n",
      "|Year|AREA NAME  |ClosedCasePercentage|\n",
      "+----+-----------+--------------------+\n",
      "|2010|Rampart    |32.84713448949121   |\n",
      "|2010|Olympic    |31.515289821999087  |\n",
      "|2010|Harbor     |29.36028339237341   |\n",
      "|2011|Olympic    |35.040060090135206  |\n",
      "|2011|Rampart    |32.4964471814306    |\n",
      "|2011|Harbor     |28.51336246316431   |\n",
      "|2012|Olympic    |34.29708533302119   |\n",
      "|2012|Rampart    |32.46000463714352   |\n",
      "|2012|Harbor     |29.509585848956675  |\n",
      "|2013|Olympic    |33.58217940999398   |\n",
      "|2013|Rampart    |32.1060382916053    |\n",
      "|2013|Harbor     |29.723638951488557  |\n",
      "|2014|Van Nuys   |32.0215235281705    |\n",
      "|2014|West Valley|31.49754809505847   |\n",
      "|2014|Mission    |31.224939855653567  |\n",
      "|2015|Van Nuys   |32.265140677157845  |\n",
      "|2015|Mission    |30.463762673676303  |\n",
      "|2015|Foothill   |30.353001803658852  |\n",
      "|2016|Van Nuys   |32.194518462124094  |\n",
      "|2016|West Valley|31.40146437042384   |\n",
      "|2016|Foothill   |29.908647228131645  |\n",
      "|2017|Van Nuys   |32.0554272517321    |\n",
      "|2017|Mission    |31.055387158996968  |\n",
      "|2017|Foothill   |30.469700657094183  |\n",
      "|2018|Foothill   |30.731346958877126  |\n",
      "|2018|Mission    |30.727023319615913  |\n",
      "|2018|Van Nuys   |28.905206942590123  |\n",
      "|2019|Mission    |30.727411112319235  |\n",
      "|2019|West Valley|30.57974335472044   |\n",
      "|2019|Foothill   |29.208050182958704  |\n",
      "|2020|West Valley|30.771131982204647  |\n",
      "|2020|Mission    |30.14974649215894   |\n",
      "|2020|Harbor     |29.693486590038315  |\n",
      "|2021|Mission    |30.318115590092276  |\n",
      "|2021|West Valley|28.971087440009363  |\n",
      "|2021|Foothill   |27.993757094211126  |\n",
      "|2022|West Valley|26.536367172306498  |\n",
      "|2022|Harbor     |26.337538060026098  |\n",
      "|2022|Topanga    |26.234013317831096  |\n",
      "|2023|Foothill   |26.76076020122974   |\n",
      "|2023|Topanga    |26.538022616453986  |\n",
      "|2023|Mission    |25.662731120516817  |\n",
      "|2024|Foothill   |18.620882188721385  |\n",
      "|2024|77th Street|17.586318167150694  |\n",
      "|2024|Mission    |17.126725219573398  |\n",
      "+----+-----------+--------------------+"
     ]
    }
   ],
   "source": [
    "# Save the combined dataset as Parquet to S3\n",
    "parquet_path = \"s3://groups-bucket-dblab-905418150721/group16/combined_crime.parquet\"\n",
    "\n",
    "combined_crime_df_1.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "# Measure execution time for reading from Parquet\n",
    "start_parquet_execution = time.time()\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_df = spark.read.format(\"parquet\").load(parquet_path)\n",
    "\n",
    "result_df_from_parquet = stations_df.join(\n",
    "    parquet_df,\n",
    "    lower(stations_df['DIVISION']) == lower(parquet_df['AREA NAME']),\n",
    "    'inner'\n",
    ")\n",
    "\n",
    "df_with_year_parquet = result_df_from_parquet.withColumn(\"Year\", year(to_date(col(\"DATE OCC\"), \"MM/dd/yyyy\")))\n",
    "\n",
    "df_with_year_resolved_parquet = df_with_year_parquet.filter(\n",
    "    (col(\"Status Desc\") != \"UNK\") & (col(\"Status Desc\") != \"Invest Cont\")\n",
    ")\n",
    "\n",
    "total_cases_parquet = df_with_year_parquet.groupBy(\"Year\", \"AREA NAME\").agg(\n",
    "    F.count(\"*\").alias(\"TotalCases\")\n",
    ")\n",
    "\n",
    "closed_cases_parquet = df_with_year_resolved_parquet.groupBy(\"Year\", \"AREA NAME\").agg(\n",
    "    F.count(\"*\").alias(\"ClosedCases\")\n",
    ")\n",
    "\n",
    "cases_parquet = total_cases_parquet.join(closed_cases_parquet, [\"Year\", \"AREA NAME\"], \"left\").withColumn(\n",
    "    \"ClosedCasePercentage\", \n",
    "    (F.col(\"ClosedCases\") / F.col(\"TotalCases\")) * 100\n",
    ")\n",
    "\n",
    "ranked_parquet = cases_parquet.withColumn(\n",
    "    \"Rank\", F.row_number().over(Window.partitionBy(\"Year\").orderBy(F.col(\"ClosedCasePercentage\").desc()))\n",
    ")\n",
    "\n",
    "top_3_parquet = ranked_parquet.filter(F.col(\"Rank\") <= 3).orderBy(\"Year\", \"Rank\")\n",
    "\n",
    "result_parquet = top_3_parquet.select(\"Year\", \"AREA NAME\", \"ClosedCasePercentage\")\n",
    "\n",
    "end_parquet_execution = time.time()\n",
    "parquet_execution_time = end_parquet_execution - start_parquet_execution\n",
    "\n",
    "print(f\"Execution Time (Parquet): {parquet_execution_time:.2f} seconds\")\n",
    "\n",
    "result_parquet.show(70, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9a3d7-d842-46a1-8516-377a1cab7257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
