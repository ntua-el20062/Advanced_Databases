{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eedad28-0259-48a7-9a9e-ff228694fec3",
   "metadata": {},
   "source": [
    "# Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2cfb43d-c0ef-4e93-b57b-0d2d4557cd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2455</td><td>application_1732639283265_2414</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2414/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-174.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2414_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, sum, collect_list, regexp_replace, first\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import min, count, avg\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5622d753-bf45-43b8-97f0-4b5ac8f0b1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def measure_join_performance(df, description):\n",
    "    start_time = time.time()\n",
    "    df.count()  # Trigger the join\n",
    "    end_time = time.time()\n",
    "    print(f\"{description} took {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75391d0-7b4c-4ea2-901e-6101f26944e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Load the crime data, LA income and RE codes csv \n",
    "crime_data_1 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True,)\n",
    "crime_data_2 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", header=True, inferSchema=True,)\n",
    "la_income = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "police_stations = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\", header=True, inferSchema=True)\n",
    "explanation = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks_fields.csv\", header=True, inferSchema=True)\n",
    "crime_data = crime_data_1.union(crime_data_2)\n",
    "\n",
    "\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_blocks_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10cc5753-f505-444b-b304-9e4076854a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out unpopulated regions or regions without a name\n",
    "populated_blocks = flattened_blocks_df.filter((flattened_blocks_df.COMM != \" \"))\n",
    "\n",
    "# Calculate total population per region-Zip Code pair (e.g., \"Community-Zip Code\", \"COMM-ZCTA10\")\n",
    "comm_pop = populated_blocks.groupBy(\"COMM\", \"ZCTA10\").agg(\n",
    "    sum(\"POP_2010\").alias(\"total_population\"),\n",
    "    sum(\"HOUSING10\").alias(\"total_households\"),\n",
    "    ST_Union_Aggr(\"geometry\").alias(\"geometry_array\")\n",
    ")\n",
    "# Clean the 'Estimated Median Income' column from $ characters\n",
    "la_income_clean = la_income.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \"[^0-9]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Join the dataframe containing the total_population and total_housing with the income\n",
    "comm_income = comm_pop.join(\n",
    "    la_income_clean, \n",
    "    ((la_income_clean[\"Community\"].contains(comm_pop[\"COMM\"])) \n",
    "      & (comm_pop[\"ZCTA10\"] == la_income_clean[\"Zip Code\"])), \"inner\"\n",
    ").drop(la_income_clean[\"Community\"])\n",
    "\n",
    "comm_income_first = comm_income.select(\n",
    "    col(\"COMM\").alias(\"region\"),\n",
    "    col(\"ZCTA10\").alias(\"Zip Code\"),\n",
    "    \"total_population\",\n",
    "    col(\"Estimated Median Income\").alias(\"median_income\"),\n",
    "    \"total_households\"\n",
    ")\n",
    "\n",
    "# Keep only the columns of interest in the comm_income\n",
    "comm_income_final = comm_income_first.groupBy(\"region\").agg(\n",
    "    sum(col(\"total_population\")).alias(\"region_total_population_income\"),\n",
    "    sum(col(\"total_households\")*col(\"median_income\")).alias(\"region_total_household_income\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf4336-029f-47b9-87c5-828fa34862d6",
   "metadata": {},
   "source": [
    "## Crimes Per Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27ffb04c-7c26-4ce3-9a28-670c2f0d9ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create geometry column for crime data\n",
    "crime_data = crime_data.filter((col(\"LON\") != 0) | (col(\"LAT\") != 0)) \n",
    "crime_data_geom = crime_data.withColumn(\"geom\", ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "crime_data_location = crime_data_geom.join(comm_pop, ST_Within(crime_data_geom[\"geom\"], comm_pop[\"geometry_array\"]), \"inner\")\n",
    "\n",
    "# Group the crime_pop dataframe by \"COMM\" column, and count the number of crimes commited per region.\n",
    "crime_num = crime_data_location.groupBy(\"COMM\", \"ZCTA10\").agg(\n",
    "    first(\"total_population\").alias(\"total_population\"),\n",
    "    first(\"total_households\").alias(\"total_households\"),\n",
    "    count(\"*\").alias(\"total_crime_number\")\n",
    ")\n",
    "\n",
    "# Calculate total region population\n",
    "region_totals = crime_num.groupBy(\"COMM\").agg(\n",
    "    sum(\"total_population\").alias(\"region_total_population_crime\"),\n",
    "    sum(\"total_crime_number\").alias(\"region_total_crime_number\")\n",
    ")\n",
    "\n",
    "\n",
    "income_result = comm_income_final.withColumn(\n",
    "    \"income_per_person\",\n",
    "    (col(\"region_total_household_income\") / col(\"region_total_population_income\"))\n",
    ")\n",
    "\n",
    "crime_result = region_totals.withColumn(\n",
    "    \"crimes_per_person\",\n",
    "    (col(\"region_total_crime_number\")/col(\"region_total_population_crime\"))\n",
    ")\n",
    "\n",
    "result = income_result.join(\n",
    "    crime_result,\n",
    "    (crime_result[\"COMM\"] == income_result[\"region\"]), \"inner\"\n",
    ").select(\"region\", \"income_per_person\", \"crimes_per_person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706a900-57d5-4356-ab16-a87d97f9e5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38be3efa1e3f40fbb84c12c0b9e31aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result.orderBy(col(\"income_per_person\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1a991-3874-4f5c-991f-15f72a1b7bde",
   "metadata": {},
   "source": [
    "## Testing \n",
    " Find the join strategies chosen by the Catalyst Optimizer using .explain() and test with different strategies using .hint() for every join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c46dcc9d-f3da-43c5-8ac2-e7b3f91553e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'DataFrameDropColumns [Community#604]\n",
      "+- Join Inner, (Contains(Community#604, COMM#716) AND (cast(ZCTA10#733 as int) = Zip Code#603))\n",
      "   :- Aggregate [COMM#716, ZCTA10#733], [COMM#716, ZCTA10#733, sum(POP_2010#725L) AS total_population#1755L, sum(HOUSING10#722L) AS total_households#1757L, st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None) AS geometry_array#1762]\n",
      "   :  +- Filter NOT (COMM#716 =  )\n",
      "   :     +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   :        +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "   :           +- Project [features#700]\n",
      "   :              +- Generate explode(features#692), false, [features#700]\n",
      "   :                 +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "   +- Project [Zip Code#603, Community#604, cast(regexp_replace(Estimated Median Income#605, [^0-9], , 1) as double) AS Estimated Median Income#1780]\n",
      "      +- Relation [Zip Code#603,Community#604,Estimated Median Income#605] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, ZCTA10: string, total_population: bigint, total_households: bigint, geometry_array: geometry, Zip Code: int, Estimated Median Income: double\n",
      "Project [COMM#716, ZCTA10#733, total_population#1755L, total_households#1757L, geometry_array#1762, Zip Code#603, Estimated Median Income#1780]\n",
      "+- Join Inner, (Contains(Community#604, COMM#716) AND (cast(ZCTA10#733 as int) = Zip Code#603))\n",
      "   :- Aggregate [COMM#716, ZCTA10#733], [COMM#716, ZCTA10#733, sum(POP_2010#725L) AS total_population#1755L, sum(HOUSING10#722L) AS total_households#1757L, st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None) AS geometry_array#1762]\n",
      "   :  +- Filter NOT (COMM#716 =  )\n",
      "   :     +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   :        +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "   :           +- Project [features#700]\n",
      "   :              +- Generate explode(features#692), false, [features#700]\n",
      "   :                 +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "   +- Project [Zip Code#603, Community#604, cast(regexp_replace(Estimated Median Income#605, [^0-9], , 1) as double) AS Estimated Median Income#1780]\n",
      "      +- Relation [Zip Code#603,Community#604,Estimated Median Income#605] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#716, ZCTA10#733, total_population#1755L, total_households#1757L, geometry_array#1762, Zip Code#603, Estimated Median Income#1780]\n",
      "+- Join Inner, (Contains(Community#604, COMM#716) AND (cast(ZCTA10#733 as int) = Zip Code#603))\n",
      "   :- Aggregate [COMM#716, ZCTA10#733], [COMM#716, ZCTA10#733, sum(POP_2010#725L) AS total_population#1755L, sum(HOUSING10#722L) AS total_households#1757L, st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None) AS geometry_array#1762]\n",
      "   :  +- Project [features#700.properties.COMM AS COMM#716, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.ZCTA10 AS ZCTA10#733, features#700.geometry AS geometry#703]\n",
      "   :     +- Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.properties.ZCTA10))\n",
      "   :        +- Generate explode(features#692), [0], false, [features#700]\n",
      "   :           +- Project [features#692]\n",
      "   :              +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "   :                 +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "   +- Project [Zip Code#603, Community#604, cast(regexp_replace(Estimated Median Income#605, [^0-9], , 1) as double) AS Estimated Median Income#1780]\n",
      "      +- Filter (isnotnull(Community#604) AND isnotnull(Zip Code#603))\n",
      "         +- Relation [Zip Code#603,Community#604,Estimated Median Income#605] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#716, ZCTA10#733, total_population#1755L, total_households#1757L, geometry_array#1762, Zip Code#603, Estimated Median Income#1780]\n",
      "   +- BroadcastHashJoin [cast(ZCTA10#733 as int)], [Zip Code#603], Inner, BuildRight, Contains(Community#604, COMM#716), false\n",
      "      :- ObjectHashAggregate(keys=[COMM#716, ZCTA10#733], functions=[sum(POP_2010#725L), sum(HOUSING10#722L), st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)], output=[COMM#716, ZCTA10#733, total_population#1755L, total_households#1757L, geometry_array#1762])\n",
      "      :  +- Exchange hashpartitioning(COMM#716, ZCTA10#733, 1000), ENSURE_REQUIREMENTS, [plan_id=6013]\n",
      "      :     +- ObjectHashAggregate(keys=[COMM#716, ZCTA10#733], functions=[partial_sum(POP_2010#725L), partial_sum(HOUSING10#722L), partial_st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)], output=[COMM#716, ZCTA10#733, sum#2029L, sum#2031L, buf#2033])\n",
      "      :        +- Project [features#700.properties.COMM AS COMM#716, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.ZCTA10 AS ZCTA10#733, features#700.geometry AS geometry#703]\n",
      "      :           +- Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.properties.ZCTA10))\n",
      "      :              +- Generate explode(features#692), false, [features#700]\n",
      "      :                 +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "      :                    +- FileScan geojson [features#692] Batched: false, DataFilters: [(size(features#692, true) > 0), isnotnull(features#692)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6016]\n",
      "         +- Project [Zip Code#603, Community#604, cast(regexp_replace(Estimated Median Income#605, [^0-9], , 1) as double) AS Estimated Median Income#1780]\n",
      "            +- Filter (isnotnull(Community#604) AND isnotnull(Zip Code#603))\n",
      "               +- FileScan csv [Zip Code#603,Community#604,Estimated Median Income#605] Batched: false, DataFilters: [isnotnull(Community#604), isnotnull(Zip Code#603)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "\n",
      "Broadcast Join took 19.70 seconds\n",
      "Sort-Merge Join took 11.00 seconds\n",
      "Shuffle Hash Join took 10.01 seconds\n",
      "Replicated Nested Loop Join took 10.16 seconds\n",
      "== Parsed Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- Aggregate [COMM#716, ZCTA10#733], [COMM#716, ZCTA10#733, sum(POP_2010#725L) AS total_population#1755L, sum(HOUSING10#722L) AS total_households#1757L, st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None) AS geometry_array#1762]\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, Date Rptd: string, DATE OCC: string, TIME OCC: int, AREA : int, AREA NAME: string, Rpt Dist No: int, Part 1-2: int, Crm Cd: int, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: int, Premis Desc: string, Weapon Used Cd: int, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: int, Crm Cd 2: int, Crm Cd 3: int, Crm Cd 4: int, ... 10 more fields\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- Aggregate [COMM#716, ZCTA10#733], [COMM#716, ZCTA10#733, sum(POP_2010#725L) AS total_population#1755L, sum(HOUSING10#722L) AS total_households#1757L, st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None) AS geometry_array#1762]\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Union false, false\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- Filter isnotnull(geometry_array#1762)\n",
      "   +- Aggregate [COMM#716, ZCTA10#733], [COMM#716, ZCTA10#733, sum(POP_2010#725L) AS total_population#1755L, sum(HOUSING10#722L) AS total_households#1757L, st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None) AS geometry_array#1762]\n",
      "      +- Project [features#700.properties.COMM AS COMM#716, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.ZCTA10 AS ZCTA10#733, features#700.geometry AS geometry#703]\n",
      "         +- Filter (isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  ))\n",
      "            +- Generate explode(features#692), [0], false, [features#700]\n",
      "               +- Project [features#692]\n",
      "                  +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "                     +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- RangeJoin geom#1827: geometry, geometry_array#1762: geometry, WITHIN\n",
      "   :- Union\n",
      "   :  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      "   :  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :  :     +- FileScan csv [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "   :  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      "   :     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :        +- FileScan csv [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "   +- Filter isnotnull(geometry_array#1762)\n",
      "      +- ObjectHashAggregate(keys=[COMM#716, ZCTA10#733], functions=[sum(POP_2010#725L), sum(HOUSING10#722L), st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)], output=[COMM#716, ZCTA10#733, total_population#1755L, total_households#1757L, geometry_array#1762])\n",
      "         +- Exchange hashpartitioning(COMM#716, ZCTA10#733, 1000), ENSURE_REQUIREMENTS, [plan_id=7205]\n",
      "            +- ObjectHashAggregate(keys=[COMM#716, ZCTA10#733], functions=[partial_sum(POP_2010#725L), partial_sum(HOUSING10#722L), partial_st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)], output=[COMM#716, ZCTA10#733, sum#2029L, sum#2031L, buf#2033])\n",
      "               +- Project [features#700.properties.COMM AS COMM#716, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.ZCTA10 AS ZCTA10#733, features#700.geometry AS geometry#703]\n",
      "                  +- Filter (isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  ))\n",
      "                     +- Generate explode(features#692), false, [features#700]\n",
      "                        +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "                           +- FileScan geojson [features#692] Batched: false, DataFilters: [(size(features#692, true) > 0), isnotnull(features#692)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, Date Rptd: string, DATE OCC: string, TIME OCC: int, AREA : int, AREA NAME: string, Rpt Dist No: int, Part 1-2: int, Crm Cd: int, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: int, Premis Desc: string, Weapon Used Cd: int, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: int, Crm Cd 2: int, Crm Cd 3: int, Crm Cd 4: int, ... 31 more fields\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**  , rightHint=(strategy=broadcast)\n",
      ":- Union false, false\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- Project [features#700.properties.BG10 AS BG10#709, features#700.properties.BG10FIP10 AS BG10FIP10#710, features#700.properties.BG12 AS BG12#711, features#700.properties.CB10 AS CB10#712, features#700.properties.CEN_FIP13 AS CEN_FIP13#713, features#700.properties.CITY AS CITY#714, features#700.properties.CITYCOM AS CITYCOM#715, features#700.properties.COMM AS COMM#716, features#700.properties.CT10 AS CT10#717, features#700.properties.CT12 AS CT12#718, features#700.properties.CTCB10 AS CTCB10#719, features#700.properties.HD_2012 AS HD_2012#720L, features#700.properties.HD_NAME AS HD_NAME#721, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.LA_FIP10 AS LA_FIP10#723, features#700.properties.OBJECTID AS OBJECTID#724L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.PUMA10 AS PUMA10#726, features#700.properties.SPA_2012 AS SPA_2012#727L, features#700.properties.SPA_NAME AS SPA_NAME#728, features#700.properties.SUP_DIST AS SUP_DIST#729, features#700.properties.SUP_LABEL AS SUP_LABEL#730, features#700.properties.ShapeSTArea AS ShapeSTArea#731, features#700.properties.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   +- Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.geometry))\n",
      "      +- Generate explode(features#692), [0], false, [features#700]\n",
      "         +- Project [features#692]\n",
      "            +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "               +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "BroadcastIndexJoin geom#1827: geometry, RightSide, LeftSide, Inner, WITHIN ST_WITHIN(geom#1827, geometry#703)\n",
      ":- Union\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- FileScan csv [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- FileScan csv [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "+- SpatialIndex geometry#703: geometry, RTREE, false, false\n",
      "   +- *(1) Project [features#700.properties.BG10 AS BG10#709, features#700.properties.BG10FIP10 AS BG10FIP10#710, features#700.properties.BG12 AS BG12#711, features#700.properties.CB10 AS CB10#712, features#700.properties.CEN_FIP13 AS CEN_FIP13#713, features#700.properties.CITY AS CITY#714, features#700.properties.CITYCOM AS CITYCOM#715, features#700.properties.COMM AS COMM#716, features#700.properties.CT10 AS CT10#717, features#700.properties.CT12 AS CT12#718, features#700.properties.CTCB10 AS CTCB10#719, features#700.properties.HD_2012 AS HD_2012#720L, features#700.properties.HD_NAME AS HD_NAME#721, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.LA_FIP10 AS LA_FIP10#723, features#700.properties.OBJECTID AS OBJECTID#724L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.PUMA10 AS PUMA10#726, features#700.properties.SPA_2012 AS SPA_2012#727L, features#700.properties.SPA_NAME AS SPA_NAME#728, features#700.properties.SUP_DIST AS SUP_DIST#729, features#700.properties.SUP_LABEL AS SUP_LABEL#730, features#700.properties.ShapeSTArea AS ShapeSTArea#731, features#700.properties.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "      +- *(1) Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.geometry))\n",
      "         +- *(1) Generate explode(features#692), false, [features#700]\n",
      "            +- *(1) Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "               +- FileScan geojson [features#692] Batched: false, DataFilters: [(size(features#692, true) > 0), isnotnull(features#692)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=shuffle_hash)\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, Date Rptd: string, DATE OCC: string, TIME OCC: int, AREA : int, AREA NAME: string, Rpt Dist No: int, Part 1-2: int, Crm Cd: int, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: int, Premis Desc: string, Weapon Used Cd: int, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: int, Crm Cd 2: int, Crm Cd 3: int, Crm Cd 4: int, ... 31 more fields\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=shuffle_hash)\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**  , rightHint=(strategy=shuffle_hash)\n",
      ":- Union false, false\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- Project [features#700.properties.BG10 AS BG10#709, features#700.properties.BG10FIP10 AS BG10FIP10#710, features#700.properties.BG12 AS BG12#711, features#700.properties.CB10 AS CB10#712, features#700.properties.CEN_FIP13 AS CEN_FIP13#713, features#700.properties.CITY AS CITY#714, features#700.properties.CITYCOM AS CITYCOM#715, features#700.properties.COMM AS COMM#716, features#700.properties.CT10 AS CT10#717, features#700.properties.CT12 AS CT12#718, features#700.properties.CTCB10 AS CTCB10#719, features#700.properties.HD_2012 AS HD_2012#720L, features#700.properties.HD_NAME AS HD_NAME#721, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.LA_FIP10 AS LA_FIP10#723, features#700.properties.OBJECTID AS OBJECTID#724L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.PUMA10 AS PUMA10#726, features#700.properties.SPA_2012 AS SPA_2012#727L, features#700.properties.SPA_NAME AS SPA_NAME#728, features#700.properties.SUP_DIST AS SUP_DIST#729, features#700.properties.SUP_LABEL AS SUP_LABEL#730, features#700.properties.ShapeSTArea AS ShapeSTArea#731, features#700.properties.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   +- Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.geometry))\n",
      "      +- Generate explode(features#692), [0], false, [features#700]\n",
      "         +- Project [features#692]\n",
      "            +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "               +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "RangeJoin geom#1827: geometry, geometry#703: geometry, WITHIN\n",
      ":- Union\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- FileScan csv [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- FileScan csv [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "+- *(1) Project [features#700.properties.BG10 AS BG10#709, features#700.properties.BG10FIP10 AS BG10FIP10#710, features#700.properties.BG12 AS BG12#711, features#700.properties.CB10 AS CB10#712, features#700.properties.CEN_FIP13 AS CEN_FIP13#713, features#700.properties.CITY AS CITY#714, features#700.properties.CITYCOM AS CITYCOM#715, features#700.properties.COMM AS COMM#716, features#700.properties.CT10 AS CT10#717, features#700.properties.CT12 AS CT12#718, features#700.properties.CTCB10 AS CTCB10#719, features#700.properties.HD_2012 AS HD_2012#720L, features#700.properties.HD_NAME AS HD_NAME#721, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.LA_FIP10 AS LA_FIP10#723, features#700.properties.OBJECTID AS OBJECTID#724L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.PUMA10 AS PUMA10#726, features#700.properties.SPA_2012 AS SPA_2012#727L, features#700.properties.SPA_NAME AS SPA_NAME#728, features#700.properties.SUP_DIST AS SUP_DIST#729, features#700.properties.SUP_LABEL AS SUP_LABEL#730, features#700.properties.ShapeSTArea AS ShapeSTArea#731, features#700.properties.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   +- *(1) Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.geometry))\n",
      "      +- *(1) Generate explode(features#692), false, [features#700]\n",
      "         +- *(1) Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "            +- FileScan geojson [features#692] Batched: false, DataFilters: [(size(features#692, true) > 0), isnotnull(features#692)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=merge)\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, Date Rptd: string, DATE OCC: string, TIME OCC: int, AREA : int, AREA NAME: string, Rpt Dist No: int, Part 1-2: int, Crm Cd: int, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: int, Premis Desc: string, Weapon Used Cd: int, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: int, Crm Cd 2: int, Crm Cd 3: int, Crm Cd 4: int, ... 31 more fields\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=merge)\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**  , rightHint=(strategy=merge)\n",
      ":- Union false, false\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- Project [features#700.properties.BG10 AS BG10#709, features#700.properties.BG10FIP10 AS BG10FIP10#710, features#700.properties.BG12 AS BG12#711, features#700.properties.CB10 AS CB10#712, features#700.properties.CEN_FIP13 AS CEN_FIP13#713, features#700.properties.CITY AS CITY#714, features#700.properties.CITYCOM AS CITYCOM#715, features#700.properties.COMM AS COMM#716, features#700.properties.CT10 AS CT10#717, features#700.properties.CT12 AS CT12#718, features#700.properties.CTCB10 AS CTCB10#719, features#700.properties.HD_2012 AS HD_2012#720L, features#700.properties.HD_NAME AS HD_NAME#721, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.LA_FIP10 AS LA_FIP10#723, features#700.properties.OBJECTID AS OBJECTID#724L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.PUMA10 AS PUMA10#726, features#700.properties.SPA_2012 AS SPA_2012#727L, features#700.properties.SPA_NAME AS SPA_NAME#728, features#700.properties.SUP_DIST AS SUP_DIST#729, features#700.properties.SUP_LABEL AS SUP_LABEL#730, features#700.properties.ShapeSTArea AS ShapeSTArea#731, features#700.properties.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   +- Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.geometry))\n",
      "      +- Generate explode(features#692), [0], false, [features#700]\n",
      "         +- Project [features#692]\n",
      "            +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "               +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "RangeJoin geom#1827: geometry, geometry#703: geometry, WITHIN\n",
      ":- Union\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- FileScan csv [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- FileScan csv [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "+- *(1) Project [features#700.properties.BG10 AS BG10#709, features#700.properties.BG10FIP10 AS BG10FIP10#710, features#700.properties.BG12 AS BG12#711, features#700.properties.CB10 AS CB10#712, features#700.properties.CEN_FIP13 AS CEN_FIP13#713, features#700.properties.CITY AS CITY#714, features#700.properties.CITYCOM AS CITYCOM#715, features#700.properties.COMM AS COMM#716, features#700.properties.CT10 AS CT10#717, features#700.properties.CT12 AS CT12#718, features#700.properties.CTCB10 AS CTCB10#719, features#700.properties.HD_2012 AS HD_2012#720L, features#700.properties.HD_NAME AS HD_NAME#721, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.LA_FIP10 AS LA_FIP10#723, features#700.properties.OBJECTID AS OBJECTID#724L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.PUMA10 AS PUMA10#726, features#700.properties.SPA_2012 AS SPA_2012#727L, features#700.properties.SPA_NAME AS SPA_NAME#728, features#700.properties.SUP_DIST AS SUP_DIST#729, features#700.properties.SUP_LABEL AS SUP_LABEL#730, features#700.properties.ShapeSTArea AS ShapeSTArea#731, features#700.properties.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   +- *(1) Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.geometry))\n",
      "      +- *(1) Generate explode(features#692), false, [features#700]\n",
      "         +- *(1) Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "            +- FileScan geojson [features#692] Batched: false, DataFilters: [(size(features#692, true) > 0), isnotnull(features#692)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DR_NO: int, Date Rptd: string, DATE OCC: string, TIME OCC: int, AREA : int, AREA NAME: string, Rpt Dist No: int, Part 1-2: int, Crm Cd: int, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: int, Premis Desc: string, Weapon Used Cd: int, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: int, Crm Cd 2: int, Crm Cd 3: int, Crm Cd 4: int, ... 31 more fields\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":     +- Filter (NOT (LON#482 = cast(0 as double)) OR NOT (LAT#481 = cast(0 as double)))\n",
      ":        +- Union false, false\n",
      ":           :- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":           +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- ResolvedHint (strategy=shuffle_replicate_nl)\n",
      "   +- Filter NOT (COMM#716 =  )\n",
      "      +- Project [properties#704.BG10 AS BG10#709, properties#704.BG10FIP10 AS BG10FIP10#710, properties#704.BG12 AS BG12#711, properties#704.CB10 AS CB10#712, properties#704.CEN_FIP13 AS CEN_FIP13#713, properties#704.CITY AS CITY#714, properties#704.CITYCOM AS CITYCOM#715, properties#704.COMM AS COMM#716, properties#704.CT10 AS CT10#717, properties#704.CT12 AS CT12#718, properties#704.CTCB10 AS CTCB10#719, properties#704.HD_2012 AS HD_2012#720L, properties#704.HD_NAME AS HD_NAME#721, properties#704.HOUSING10 AS HOUSING10#722L, properties#704.LA_FIP10 AS LA_FIP10#723, properties#704.OBJECTID AS OBJECTID#724L, properties#704.POP_2010 AS POP_2010#725L, properties#704.PUMA10 AS PUMA10#726, properties#704.SPA_2012 AS SPA_2012#727L, properties#704.SPA_NAME AS SPA_NAME#728, properties#704.SUP_DIST AS SUP_DIST#729, properties#704.SUP_LABEL AS SUP_LABEL#730, properties#704.ShapeSTArea AS ShapeSTArea#731, properties#704.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "         +- Project [features#700.geometry AS geometry#703, features#700.properties AS properties#704, features#700.type AS type#705]\n",
      "            +- Project [features#700]\n",
      "               +- Generate explode(features#692), false, [features#700]\n",
      "                  +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**  , rightHint=(strategy=shuffle_replicate_nl)\n",
      ":- Union false, false\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- Relation [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] csv\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- Relation [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] csv\n",
      "+- Project [features#700.properties.BG10 AS BG10#709, features#700.properties.BG10FIP10 AS BG10FIP10#710, features#700.properties.BG12 AS BG12#711, features#700.properties.CB10 AS CB10#712, features#700.properties.CEN_FIP13 AS CEN_FIP13#713, features#700.properties.CITY AS CITY#714, features#700.properties.CITYCOM AS CITYCOM#715, features#700.properties.COMM AS COMM#716, features#700.properties.CT10 AS CT10#717, features#700.properties.CT12 AS CT12#718, features#700.properties.CTCB10 AS CTCB10#719, features#700.properties.HD_2012 AS HD_2012#720L, features#700.properties.HD_NAME AS HD_NAME#721, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.LA_FIP10 AS LA_FIP10#723, features#700.properties.OBJECTID AS OBJECTID#724L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.PUMA10 AS PUMA10#726, features#700.properties.SPA_2012 AS SPA_2012#727L, features#700.properties.SPA_NAME AS SPA_NAME#728, features#700.properties.SUP_DIST AS SUP_DIST#729, features#700.properties.SUP_LABEL AS SUP_LABEL#730, features#700.properties.ShapeSTArea AS ShapeSTArea#731, features#700.properties.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   +- Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.geometry))\n",
      "      +- Generate explode(features#692), [0], false, [features#700]\n",
      "         +- Project [features#692]\n",
      "            +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "               +- Relation [crs#691,features#692,name#693,type#694] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "RangeJoin geom#1827: geometry, geometry#703: geometry, WITHIN\n",
      ":- Union\n",
      ":  :- Project [DR_NO#455, Date Rptd#456, DATE OCC#457, TIME OCC#458, AREA #459, AREA NAME#460, Rpt Dist No#461, Part 1-2#462, Crm Cd#463, Crm Cd Desc#464, Mocodes#465, Vict Age#466, Vict Sex#467, Vict Descent#468, Premis Cd#469, Premis Desc#470, Weapon Used Cd#471, Weapon Desc#472, Status#473, Status Desc#474, Crm Cd 1#475, Crm Cd 2#476, Crm Cd 3#477, Crm Cd 4#478, ... 5 more fields]\n",
      ":  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":  :     +- FileScan csv [DR_NO#455,Date Rptd#456,DATE OCC#457,TIME OCC#458,AREA #459,AREA NAME#460,Rpt Dist No#461,Part 1-2#462,Crm Cd#463,Crm Cd Desc#464,Mocodes#465,Vict Age#466,Vict Sex#467,Vict Descent#468,Premis Cd#469,Premis Desc#470,Weapon Used Cd#471,Weapon Desc#472,Status#473,Status Desc#474,Crm Cd 1#475,Crm Cd 2#476,Crm Cd 3#477,Crm Cd 4#478,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      ":  +- Project [DR_NO#529, Date Rptd#530, DATE OCC#531, TIME OCC#532, AREA#533, AREA NAME#534, Rpt Dist No#535, Part 1-2#536, Crm Cd#537, Crm Cd Desc#538, Mocodes#539, Vict Age#540, Vict Sex#541, Vict Descent#542, Premis Cd#543, Premis Desc#544, Weapon Used Cd#545, Weapon Desc#546, Status#547, Status Desc#548, Crm Cd 1#549, Crm Cd 2#550, Crm Cd 3#551, Crm Cd 4#552, ... 5 more fields]\n",
      ":     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      ":        +- FileScan csv [DR_NO#529,Date Rptd#530,DATE OCC#531,TIME OCC#532,AREA#533,AREA NAME#534,Rpt Dist No#535,Part 1-2#536,Crm Cd#537,Crm Cd Desc#538,Mocodes#539,Vict Age#540,Vict Sex#541,Vict Descent#542,Premis Cd#543,Premis Desc#544,Weapon Used Cd#545,Weapon Desc#546,Status#547,Status Desc#548,Crm Cd 1#549,Crm Cd 2#550,Crm Cd 3#551,Crm Cd 4#552,... 4 more fields] Batched: false, DataFilters: [(NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "+- *(1) Project [features#700.properties.BG10 AS BG10#709, features#700.properties.BG10FIP10 AS BG10FIP10#710, features#700.properties.BG12 AS BG12#711, features#700.properties.CB10 AS CB10#712, features#700.properties.CEN_FIP13 AS CEN_FIP13#713, features#700.properties.CITY AS CITY#714, features#700.properties.CITYCOM AS CITYCOM#715, features#700.properties.COMM AS COMM#716, features#700.properties.CT10 AS CT10#717, features#700.properties.CT12 AS CT12#718, features#700.properties.CTCB10 AS CTCB10#719, features#700.properties.HD_2012 AS HD_2012#720L, features#700.properties.HD_NAME AS HD_NAME#721, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.LA_FIP10 AS LA_FIP10#723, features#700.properties.OBJECTID AS OBJECTID#724L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.PUMA10 AS PUMA10#726, features#700.properties.SPA_2012 AS SPA_2012#727L, features#700.properties.SPA_NAME AS SPA_NAME#728, features#700.properties.SUP_DIST AS SUP_DIST#729, features#700.properties.SUP_LABEL AS SUP_LABEL#730, features#700.properties.ShapeSTArea AS ShapeSTArea#731, features#700.properties.ShapeSTLength AS ShapeSTLength#732, ... 2 more fields]\n",
      "   +- *(1) Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.geometry))\n",
      "      +- *(1) Generate explode(features#692), false, [features#700]\n",
      "         +- *(1) Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "            +- FileScan geojson [features#692] Batched: false, DataFilters: [(size(features#692, true) > 0), isnotnull(features#692)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "Broadcast Join - crime_data_location took 35.54 seconds\n",
      "Sort-Merge Join - crime_data_location took 33.84 seconds\n",
      "Shuffle Hash Join - crime_data_location took 28.70 seconds\n",
      "Replicated Nested Loop Join - crime_data_location took 23.11 seconds\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [region#1807, income_per_person#1982, crimes_per_person#1987]\n",
      "   +- SortMergeJoin [region#1807], [COMM#716], Inner\n",
      "      :- Sort [region#1807 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[region#1807], functions=[sum((cast(total_households#1757L as double) * median_income#1809)), sum(total_population#1755L)], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(region#1807, 1000), ENSURE_REQUIREMENTS, [plan_id=8222]\n",
      "      :        +- HashAggregate(keys=[region#1807], functions=[partial_sum((cast(total_households#1757L as double) * median_income#1809)), partial_sum(total_population#1755L)], schema specialized)\n",
      "      :           +- Project [COMM#716 AS region#1807, total_population#1755L, Estimated Median Income#1780 AS median_income#1809, total_households#1757L]\n",
      "      :              +- BroadcastHashJoin [cast(ZCTA10#733 as int)], [Zip Code#603], Inner, BuildRight, Contains(Community#604, COMM#716), false\n",
      "      :                 :- HashAggregate(keys=[COMM#716, ZCTA10#733], functions=[sum(POP_2010#725L), sum(HOUSING10#722L)], schema specialized)\n",
      "      :                 :  +- Exchange hashpartitioning(COMM#716, ZCTA10#733, 1000), ENSURE_REQUIREMENTS, [plan_id=8214]\n",
      "      :                 :     +- HashAggregate(keys=[COMM#716, ZCTA10#733], functions=[partial_sum(POP_2010#725L), partial_sum(HOUSING10#722L)], schema specialized)\n",
      "      :                 :        +- Project [features#700.properties.COMM AS COMM#716, features#700.properties.HOUSING10 AS HOUSING10#722L, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.ZCTA10 AS ZCTA10#733]\n",
      "      :                 :           +- Filter ((isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  )) AND isnotnull(features#700.properties.ZCTA10))\n",
      "      :                 :              +- Generate explode(features#692), false, [features#700]\n",
      "      :                 :                 +- Filter ((size(features#692, true) > 0) AND isnotnull(features#692))\n",
      "      :                 :                    +- FileScan geojson [features#692] Batched: false, DataFilters: [(size(features#692, true) > 0), isnotnull(features#692)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "      :                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=8217]\n",
      "      :                    +- Project [Zip Code#603, Community#604, cast(regexp_replace(Estimated Median Income#605, [^0-9], , 1) as double) AS Estimated Median Income#1780]\n",
      "      :                       +- Filter (isnotnull(Community#604) AND isnotnull(Zip Code#603))\n",
      "      :                          +- FileScan csv [Zip Code#603,Community#604,Estimated Median Income#605] Batched: false, DataFilters: [isnotnull(Community#604), isnotnull(Zip Code#603)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "      +- Sort [COMM#716 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#716], functions=[sum(total_crime_number#1964L), sum(total_population#1960L)], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#716, 1000), ENSURE_REQUIREMENTS, [plan_id=8235]\n",
      "               +- HashAggregate(keys=[COMM#716], functions=[partial_sum(total_crime_number#1964L), partial_sum(total_population#1960L)], schema specialized)\n",
      "                  +- HashAggregate(keys=[COMM#716, ZCTA10#733], functions=[first(total_population#1755L, false), count(1)], schema specialized)\n",
      "                     +- Exchange hashpartitioning(COMM#716, ZCTA10#733, 1000), ENSURE_REQUIREMENTS, [plan_id=8231]\n",
      "                        +- HashAggregate(keys=[COMM#716, ZCTA10#733], functions=[partial_first(total_population#1755L, false), partial_count(1)], schema specialized)\n",
      "                           +- Project [COMM#716, ZCTA10#733, total_population#1755L]\n",
      "                              +- RangeJoin geom#1827: geometry, geometry_array#1762: geometry, WITHIN\n",
      "                                 :- Union\n",
      "                                 :  :- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#1827]\n",
      "                                 :  :  +- Filter ((NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                 :  :     +- FileScan csv [LAT#481,LON#482] Batched: false, DataFilters: [(NOT (LON#482 = 0.0) OR NOT (LAT#481 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<LAT:double,LON:double>\n",
      "                                 :  +- Project [ **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geom#2933]\n",
      "                                 :     +- Filter ((NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                                 :        +- FileScan csv [LAT#555,LON#556] Batched: false, DataFilters: [(NOT (LON#556 = 0.0) OR NOT (LAT#555 = 0.0)), isnotnull( **org.apache.spark.sql.sedona_sql.expre..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [Or(Not(EqualTo(LON,0.0)),Not(EqualTo(LAT,0.0)))], ReadSchema: struct<LAT:double,LON:double>\n",
      "                                 +- Filter isnotnull(geometry_array#1762)\n",
      "                                    +- ObjectHashAggregate(keys=[COMM#716, ZCTA10#733], functions=[sum(POP_2010#725L), st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                                       +- Exchange hashpartitioning(COMM#716, ZCTA10#733, 1000), ENSURE_REQUIREMENTS, [plan_id=8224]\n",
      "                                          +- ObjectHashAggregate(keys=[COMM#716, ZCTA10#733], functions=[partial_sum(POP_2010#725L), partial_st_union_aggr(geometry#703, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@54c5b8bd, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, None)])\n",
      "                                             +- Project [features#700.properties.COMM AS COMM#716, features#700.properties.POP_2010 AS POP_2010#725L, features#700.properties.ZCTA10 AS ZCTA10#733, features#700.geometry AS geometry#703]\n",
      "                                                +- Filter (isnotnull(features#700.properties.COMM) AND NOT (features#700.properties.COMM =  ))\n",
      "                                                   +- Generate explode(features#1993), false, [features#700]\n",
      "                                                      +- Filter ((size(features#1993, true) > 0) AND isnotnull(features#1993))\n",
      "                                                         +- FileScan geojson [features#1993] Batched: false, DataFilters: [(size(features#1993, true) > 0), isnotnull(features#1993)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG10:string,BG10FIP10:string,BG12:...\n",
      "\n",
      "\n",
      "Broadcast Join - result took 43.74 seconds\n",
      "Sort-Merge Join - result took 36.46 seconds\n",
      "Shuffle Hash Join - result took 41.33 seconds\n",
      "Replicated Nested Loop Join - result took 35.31 seconds"
     ]
    }
   ],
   "source": [
    "######################################## 1st Join ###############################\n",
    "\n",
    "# Use the \"explain\" function to find out the optimizer's choice on join strategy\n",
    "comm_income.explain(True)\n",
    "\n",
    "# Experiment with join strategies \n",
    "comm_income_broadcast = comm_pop.join(\n",
    "    la_income_clean.hint(\"broadcast\"),\n",
    "    ((la_income_clean[\"Community\"].contains(comm_pop[\"COMM\"])) \n",
    "      & (comm_pop[\"ZCTA10\"] == la_income_clean[\"Zip Code\"])), \"inner\"\n",
    ")\n",
    "\n",
    "comm_income_merge = comm_pop.join(\n",
    "    la_income_clean.hint(\"merge\"),\n",
    "   ((la_income_clean[\"Community\"].contains(comm_pop[\"COMM\"])) \n",
    "      & (comm_pop[\"ZCTA10\"] == la_income_clean[\"Zip Code\"])), \"inner\"\n",
    ")\n",
    "\n",
    "\n",
    "comm_income_shuffle_hash = comm_pop.join(\n",
    "    la_income_clean.hint(\"shuffle_hash\"),\n",
    "  ((la_income_clean[\"Community\"].contains(comm_pop[\"COMM\"])) \n",
    "      & (comm_pop[\"ZCTA10\"] == la_income_clean[\"Zip Code\"])), \"inner\"\n",
    ")\n",
    "\n",
    "comm_income_shuffle_replicate_nl = comm_pop.join(\n",
    "    la_income_clean.hint(\"shuffle_replicate_nl\"),\n",
    "   ((la_income_clean[\"Community\"].contains(comm_pop[\"COMM\"])) \n",
    "      & (comm_pop[\"ZCTA10\"] == la_income_clean[\"Zip Code\"])), \"inner\"\n",
    ")\n",
    "\n",
    "# Measure their performances \n",
    "measure_join_performance(comm_income_broadcast, \"Broadcast Join\")\n",
    "measure_join_performance(comm_income_merge, \"Sort-Merge Join\")\n",
    "measure_join_performance(comm_income_shuffle_hash, \"Shuffle Hash Join\")\n",
    "measure_join_performance(comm_income_shuffle_replicate_nl, \"Replicated Nested Loop Join\")\n",
    "\n",
    "######################################## 2nd Join ###############################\n",
    "\n",
    "crime_data_location.explain(True)\n",
    "\n",
    "crime_data_location_broadcast = crime_data_geom.join(populated_blocks.hint(\"broadcast\"), ST_Within(crime_data_geom[\"geom\"], flattened_blocks_df[\"geometry\"]), \"inner\")\n",
    "crime_data_location_broadcast.explain(True)\n",
    "\n",
    "crime_data_location_shuffle_hash = crime_data_geom.join(populated_blocks.hint(\"shuffle_hash\"), ST_Within(crime_data_geom[\"geom\"], flattened_blocks_df[\"geometry\"]), \"inner\")\n",
    "crime_data_location_shuffle_hash.explain(True)\n",
    "\n",
    "crime_data_location_merge = crime_data_geom.join(populated_blocks.hint(\"merge\"), ST_Within(crime_data_geom[\"geom\"], flattened_blocks_df[\"geometry\"]), \"inner\")\n",
    "crime_data_location_merge.explain(True)\n",
    "\n",
    "crime_data_location_shuffle_replicate_nl = crime_data_geom.join(populated_blocks.hint(\"shuffle_replicate_nl\"), ST_Within(crime_data_geom[\"geom\"], flattened_blocks_df[\"geometry\"]), \"inner\")\n",
    "crime_data_location_shuffle_replicate_nl.explain(True)\n",
    "\n",
    "\n",
    "# Measure Performances\n",
    "measure_join_performance(crime_data_location_broadcast, \"Broadcast Join - crime_data_location\")\n",
    "measure_join_performance(crime_data_location_shuffle_hash, \"Sort-Merge Join - crime_data_location\")\n",
    "measure_join_performance(crime_data_location_merge, \"Shuffle Hash Join - crime_data_location\")\n",
    "measure_join_performance(crime_data_location_shuffle_replicate_nl, \"Replicated Nested Loop Join - crime_data_location\")\n",
    "\n",
    "\n",
    "######################################## 3rd Join ###############################\n",
    "result.explain()\n",
    "# Experiment with join strategies \n",
    "result_broadcast = income_result.join(\n",
    "    crime_result.hint(\"broadcast\"),\n",
    "    (crime_result[\"COMM\"] == income_result[\"region\"]), \"inner\"\n",
    ")\n",
    "\n",
    "result_shuffle_hash = income_result.join(\n",
    "    crime_result.hint(\"shuffle_hash\"), \n",
    "    (crime_result[\"COMM\"] == income_result[\"region\"]), \"inner\"\n",
    ")\n",
    "\n",
    "result_merge = income_result.join(\n",
    "    crime_result.hint(\"merge\"), \n",
    "    (crime_result[\"COMM\"] == income_result[\"region\"]), \"inner\"\n",
    ")\n",
    "\n",
    "result_shuffle_replicate_nl = income_result.join(\n",
    "    crime_result.hint(\"shuffle_replicate_nl\"), \n",
    "    (crime_result[\"COMM\"] == income_result[\"region\"]), \"inner\"\n",
    ")\n",
    "\n",
    "# Measure Performances\n",
    "measure_join_performance(result_broadcast, \"Broadcast Join - result\")\n",
    "measure_join_performance(result_merge, \"Sort-Merge Join - result\")\n",
    "measure_join_performance(result_merge, \"Shuffle Hash Join - result\")\n",
    "measure_join_performance(result_shuffle_replicate_nl, \"Replicated Nested Loop Join - result\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f330923e-4a23-44a5-980d-95e1bbe9b74c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in result is: 163"
     ]
    }
   ],
   "source": [
    "row_count = result.count()\n",
    "print(f\"The number of rows in result is: {row_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
